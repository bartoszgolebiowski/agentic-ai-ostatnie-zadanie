# -*- coding: utf-8 -*-
"""
LLM-as-Judge Evaluator - dynamic model creation and evaluation.

Based on lesson 5.3 methodology using pydantic.create_model.
"""

from pydantic import BaseModel, Field, create_model
from typing import List, Type, Any, Dict
from .leaderboard_parser import CheckDefinition
from engine.client import get_llm_client
from config import Config


def create_evaluation_model(checks: List[CheckDefinition]) -> Type[BaseModel]:
    """
    Dynamically create Pydantic model for evaluation.

    For each check, create 2 fields:
    - {id}_reasoning: str (Chain of Thought with quoted dialog fragments)
    - {id}_passed: bool (Binary verdict)

    Args:
        checks: List of CheckDefinition to evaluate

    Returns:
        Dynamically created Pydantic model class

    Example:
        checks = [CheckDefinition(id="LC-001", ...)]
        Model = create_evaluation_model(checks)
        # Model has fields: LC_001_reasoning, LC_001_passed
    """
    fields = {}

    for check in checks:
        # Normalize ID: LC-001 -> LC_001 (pydantic field name)
        prefix = check.id.replace("-", "_")

        # Field 1: reasoning (Chain of Thought)
        fields[f"{prefix}_reasoning"] = (
            str,
            Field(
                ...,
                description=(
                    f"Analiza dla {check.id} ({check.title}). "
                    f"Kryterium: {check.description}. "
                    f"WAÅ»NE: Zacytuj konkretny fragment dialogu jako dowÃ³d "
                    f"lub napisz 'Brak dowodu w dialogu' jeÅ›li kryterium nie wystÄ™puje."
                ),
            ),
        )

        # Field 2: passed (Binary verdict)
        fields[f"{prefix}_passed"] = (
            bool,
            Field(
                ...,
                description=(
                    f"Werdykt binarny dla {check.id}. "
                    f"True jeÅ›li kryterium w peÅ‚ni speÅ‚nione (wszystkie elementy obecne). "
                    f"False jeÅ›li wykryto bÅ‚Ä…d, naruszenie lub brak wymaganego elementu."
                ),
            ),
        )

    # Create model dynamically
    return create_model("DynamicEvaluationResult", **fields)


def evaluate_conversation(
    session_state: Dict[str, Any], checks: List[CheckDefinition]
) -> Dict[str, Any]:
    """
    Evaluate a full session state using LLM-as-Judge with structured output.

    Args:
        session_state: Serialized SessionState dict containing conversation_history
        checks: List of criteria to evaluate (already filtered by priority)

    Returns:
        Dict with:
        - results: List of {id, title, reasoning, passed}
        - summary: {passed_count, failed_count, total, score_pct}
        - priority: Priority group evaluated
    """
    conversation_history = (
        session_state.get("conversation_history", []) if session_state else []
    )

    if not checks:
        return {
            "error": "No checks provided for evaluation",
            "results": [],
            "summary": {},
        }

    if not conversation_history:
        return {"error": "No conversation to evaluate", "results": [], "summary": {}}

    # Create dynamic model
    EvaluationModel = create_evaluation_model(checks)

    # Format conversation for LLM
    dialog_text = "\n\n".join(
        [f"**{msg['role'].upper()}**: {msg['content']}" for msg in conversation_history]
    )

    # Build system prompt
    system_prompt = """JesteÅ› ekspertem od coachingu i sÄ™dziÄ… oceniajÄ…cym jakoÅ›Ä‡ sesji coachingowych.

Twoim zadaniem jest przeanalizowaÄ‡ dialog i oceniÄ‡ kaÅ¼de kryterium wedÅ‚ug podanych instrukcji.

Zasady oceny:
1. Dla kaÅ¼dego kryterium napisz uzasadnienie (reasoning) cytujÄ…c konkretny fragment dialogu
2. JeÅ›li nie znajdziesz dowodu w dialogu, napisz "Brak dowodu w dialogu"
3. Wydaj werdykt (passed): True jeÅ›li kryterium CAÅKOWICIE speÅ‚nione, False w przeciwnym razie
4. BÄ…dÅº obiektywny i surowy - czÄ™Å›ciowe speÅ‚nienie = False
5. Cytuj DOKÅADNIE to co powiedziano, nie parafrazuj

PamiÄ™taj: To ocena JAKOÅšCI coachingu, nie iloÅ›ci tekstu."""

    # Build user prompt
    criteria_list = "\n".join(
        [
            f"{i+1}. {check.id} - {check.title} ({check.priority}): {check.description}"
            for i, check in enumerate(checks)
        ]
    )

    user_prompt = f"""## Dialog do oceny:

{dialog_text}

---

## Kryteria do oceny ({len(checks)} total):

{criteria_list}

---

OceÅ„ powyÅ¼szy dialog wedÅ‚ug wszystkich {len(checks)} kryteriÃ³w."""

    # Call LLM with structured output
    client = get_llm_client()

    try:
        result = client.chat.completions.create(
            model=Config.MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            response_model=EvaluationModel,
            temperature=0.0,  # Deterministic evaluation
        )
    except Exception as e:
        return {
            "error": f"LLM evaluation failed: {str(e)}",
            "results": [],
            "summary": {},
        }

    # Parse results
    results = []
    passed_count = 0

    for check in checks:
        prefix = check.id.replace("-", "_")
        reasoning = getattr(result, f"{prefix}_reasoning", "")
        passed = getattr(result, f"{prefix}_passed", False)

        if passed:
            passed_count += 1

        results.append(
            {
                "id": check.id,
                "title": check.title,
                "priority": check.priority,
                "reasoning": reasoning,
                "passed": passed,
            }
        )

    failed_count = len(checks) - passed_count
    score_pct = (passed_count / len(checks) * 100) if checks else 0

    # Determine priority group
    priority_group = checks[0].priority if checks else "UNKNOWN"
    if len(set(c.priority for c in checks)) > 1:
        priority_group = "ALL"

    summary = {
        "passed_count": passed_count,
        "failed_count": failed_count,
        "total": len(checks),
        "score_pct": round(score_pct, 1),
        "priority": priority_group,
    }

    return {"results": results, "summary": summary}


def format_evaluation_results(eval_result: Dict[str, Any]) -> str:
    """
    Format evaluation results as human-readable text for Gradio display.

    Args:
        eval_result: Output from evaluate_conversation()

    Returns:
        Formatted string with emoji indicators
    """
    if "error" in eval_result:
        return f"âŒ **Error:** {eval_result['error']}"

    summary = eval_result["summary"]
    results = eval_result["results"]

    # Header
    output = f"""ğŸ¯ **Evaluation Results**

**Priority Group:** {summary['priority']}
**Score:** {summary['passed_count']}/{summary['total']} ({summary['score_pct']}%)
**Status:** {'âœ… PASS' if summary['score_pct'] == 100 else 'âš ï¸ PARTIAL' if summary['score_pct'] > 0 else 'âŒ FAIL'}

---

## Detailed Results:

"""

    # Individual results
    for r in results:
        icon = "âœ…" if r["passed"] else "âŒ"
        output += f"""
### {icon} {r['id']} - {r['title']} ({r['priority']})

**Verdict:** {'PASS âœ…' if r['passed'] else 'FAIL âŒ'}

**Reasoning:**
{r['reasoning']}

---
"""

    return output
